{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [1], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mplotly\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexpress\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpx\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mseaborn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01msns\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpathlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Path\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset  # Ensure Dataset is imported\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import cv2\n",
    "from matplotlib.image import imread\n",
    "\n",
    "import tensorflow as tf\n",
    "#from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing import image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import glob\n",
    "import PIL\n",
    "import random\n",
    "\n",
    "random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set parameters\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 25\n",
    "DATA_PATH = 'C:/Users/moham/OneDrive - Virginia Tech/Summer-Fall 2024/CS 5805/ML Project/CODE/All Images'  # Update this to the dataset path\n",
    "\n",
    "# Load and preprocess images\n",
    "def load_images(data_path):\n",
    "    images, labels = [], []\n",
    "    classes = {'benign': 0, 'malignant': 1}  # Map class names to integers\n",
    "\n",
    "    for label in os.listdir(data_path):\n",
    "        class_path = os.path.join(data_path, label)\n",
    "        if os.path.isdir(class_path):\n",
    "            for img_name in tqdm(os.listdir(class_path), desc=f\"Loading {label} images\"):\n",
    "                img_path = os.path.join(class_path, img_name)\n",
    "                try:\n",
    "                    img = cv2.imread(img_path)\n",
    "                    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "                    images.append(img)\n",
    "                    labels.append(classes[label])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {img_path}: {e}\")\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Load images\n",
    "images, labels = load_images(DATA_PATH)\n",
    "images = images / 255.0  # Normalize pixel values to [0, 1]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(rotation_range=15,\n",
    "                             width_shift_range=0.1,\n",
    "                             height_shift_range=0.1,\n",
    "                             horizontal_flip=True)\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Build the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(datagen.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=EPOCHS)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation Loss: {loss:.4f}, Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"breakhis_cnn_model.h5\")\n",
    "print(\"Model saved as breakhis_cnn_model.h5\")\n",
    "\n",
    "# Visualize training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Model Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Model Loss')\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Not used\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path to the folder\n",
    "folder_path = destination_dir\n",
    "\n",
    "# Create dictionaries to store file information\n",
    "patient_ID = {}\n",
    "patient_Diag = {}\n",
    "\n",
    "# Loop through each file in the folder to extract IDs and Diagnoses\n",
    "for i, filename in enumerate(os.listdir(folder_path), start=1):\n",
    "    if os.path.isfile(os.path.join(folder_path, filename)):\n",
    "        # Split filename and extension\n",
    "        name, ext = os.path.splitext(filename)\n",
    "\n",
    "        # Extract Patient ID (e.g., characters 2 to 6)\n",
    "        trimmed_name = name[4:]  # Adjust slicing as needed\n",
    "        truncated_id = trimmed_name[:1]\n",
    "        patient_Diag[i] = truncated_id\n",
    "\n",
    "        # Extract Patient Diagnosis (e.g., characters 6 onward)\n",
    "        diag_name = name[8:]\n",
    "        truncated_diag = diag_name[:16]\n",
    "        patient_ID[i] = truncated_diag\n",
    "\n",
    "# Convert dictionaries to lists\n",
    "id_list = list(patient_ID.values())\n",
    "diag_list = list(patient_Diag.values())\n",
    "\n",
    "# Assuming `features_list` is already created from feature extraction\n",
    "df_features = pd.DataFrame(features_list, columns=columns)\n",
    "\n",
    "# Add patient_ID and patient_Diag as columns to the DataFrame\n",
    "df_features['patient_ID'] = id_list\n",
    "df_features['patient_Diag'] = diag_list\n",
    "\n",
    "# Save to CSV or print the DataFrame\n",
    "df_features.to_csv(\"BreaKHis_full_extracted_features.csv\", index=False)\n",
    "print(df_features)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Bagging Classifier\n",
    "class BaggingClassifier:\n",
    "    def __init__(self, base_estimator, n_estimators=10, random_state=None):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.random_state = random_state\n",
    "        self.models = []\n",
    "        self.samples = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        np.random.seed(self.random_state)\n",
    "        self.models = []\n",
    "        self.samples = []\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Bootstrap sampling\n",
    "            indices = np.random.choice(range(len(X)), size=len(X), replace=True)\n",
    "            X_sample, y_sample = X[indices], y[indices]\n",
    "            self.samples.append(indices)\n",
    "\n",
    "            # Train a new model\n",
    "            model = self.base_estimator\n",
    "            model.fit(X_sample, y_sample)\n",
    "            self.models.append(model)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Aggregate predictions from all models (majority vote for classification)\n",
    "        predictions = np.array([model.predict(X) for model in self.models])\n",
    "        # Majority vote\n",
    "        final_predictions = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=predictions)\n",
    "        return final_predictions\n",
    "\n",
    "# Load a dataset\n",
    "data = df_features\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the Bagging Classifier with a DecisionTreeClassifier as the base estimator\n",
    "bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=42), n_estimators=10, random_state=42)\n",
    "\n",
    "# Train the Bagging Classifier\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = bagging_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of Bagging Classifier: {accuracy:.2f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the path to the folder\n",
    "folder_path = destination_dir\n",
    "\n",
    "# Create a dictionary to store file names\n",
    "patient_ID = {}\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for i, filename in enumerate(os.listdir(folder_path), start=1):\n",
    "    # Only add files (not directories) to the dictionary\n",
    "    if os.path.isfile(os.path.join(folder_path, filename)):\n",
    "        name, ext = os.path.splitext(filename)\n",
    "        trimmed_name = name[2:]\n",
    "        truncated_name = trimmed_name[:5]\n",
    "        patient_ID[i] = truncated_name\n",
    "resolution= {}\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for i, filename in enumerate(os.listdir(folder_path), start=1):\n",
    "    # Only add files (not directories) to the dictionary\n",
    "    if os.path.isfile(os.path.join(folder_path, filename)):\n",
    "        name, ext = os.path.splitext(filename)\n",
    "        trimmed_name = name[19:]\n",
    "        truncated_name = trimmed_name[:22]\n",
    "        resolution[i] = truncated_name\n",
    "# Output the dictionary\n",
    "\n",
    "# Create a dictionary to store file names\n",
    "patient_Diag = {}\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for i, filename in enumerate(os.listdir(folder_path), start=1):\n",
    "    # Only add files (not directories) to the dictionary\n",
    "    if os.path.isfile(os.path.join(folder_path, filename)):\n",
    "        name, ext = os.path.splitext(filename)\n",
    "        trimmed_name = name[6:]\n",
    "        truncated_name = trimmed_name[:16]\n",
    "        patient_Diag[i] = truncated_name\n",
    "\n",
    "# Output the dictionary\n",
    "print(resolution)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for label in os.listdir(DATA_PATH):\n",
    "    class_path = os.path.join(DATA_PATH, label)\n",
    "    print(os.path.isdir(class_path))\n",
    "    # os.path.isdir(class_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#To separate names\n",
    "for im in list_of_path:\n",
    "    print(im.split('.')[0].split('-')[0].split('_')[1],im.split('.')[0].split('-')[1]+'-'+im.split('.')[0].split('-')[2],im.split('.')[0].split('-')[3],im.split('.')[0].split('-')[0].split('_')[2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_path = 'BreaKHis_full_extracted_features.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "destination_dir = 'C:/Users/moham/OneDrive - Virginia Tech/Summer-Fall 2024/CS 5805/ML Project/CODE/All Images'\n",
    "list_of_path=[]\n",
    "i=0\n",
    "for root, dirs, files in os.walk(destination_dir):\n",
    "    for file in files:\n",
    "        list_of_path.append(file)\n",
    "        i+=1\n",
    "list_of_path"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load dataset\n",
    "file_path = 'BreaKHis_full_extracted_features.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "#Only 100 resolution\n",
    "# data=data.loc[data['resolution']==100]\n",
    "\n",
    "# Preprocessing\n",
    "# Drop non-numerical 'patient_ID' and handle missing values\n",
    "X = data.drop(columns=['file_name','resolution','patient_Diag', 'patient_ID','tumor_type'])\n",
    "X = X.fillna(X.mean())\n",
    "\n",
    "# Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data['patient_Diag'])\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Handle class imbalance using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Bagging Classifier with Decision Tree\n",
    "base_model = DecisionTreeClassifier(random_state=42)\n",
    "bagging_clf = BaggingClassifier(estimator=base_model, random_state=42)  # Corrected parameter name\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100,200,500],  # Number of trees\n",
    "    'estimator__max_depth': [5, 10, 15,20],  # Depth of decision trees\n",
    "    'estimator__min_samples_split': [2, 5, 10,20],  # Minimum samples for splitting\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=bagging_clf, param_grid=param_grid, cv=10, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best Model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Test Set Evaluation\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Performance Metrics\n",
    "train_accuracy = accuracy_score(X_train, best_model.predict(X_train))\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_report_str = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n",
    "\n",
    "# Output results\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report_str)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}